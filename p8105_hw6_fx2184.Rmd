---
title: "p8105_hw6_fx2184"
author: "Fei"
date: "2022-12-03"
output: github_document
---
```{r setup, include = FALSE}
library(tidyverse)
library(viridis)
library(modelr)
library(purrr)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## Problem 2 Washingtong Post - Homicides 
```{r}
#import the dataset 
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicides_raw = read_csv(url)
```

```{r warning=FALSE}
# create variable city_state and clean the dataset
homicides = homicides_raw %>% 
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, state,sep = ", "),
         victim_age = as.numeric(victim_age),
         victim_race = fct_relevel(victim_race, "White"),
         resolved = case_when(
           disposition == "Closed by arrest" ~ 1,
           disposition ==  "Open/No arrest" ~ 0,
           disposition ==  "Closed without arrest" ~ 0
         ),
         reported_date = as.Date(as.character(reported_date), format = "%Y%m%d")) %>% 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO","Tulsa, AL")))%>% 
  filter(victim_race  %in% c("White", "Black"))
```

```{r}
## generate glm model, let binary variable resolved as outcome, and age, sex and race as predictors. 
Baltimoe_fit_logistic = homicides %>% 
    filter(city_state  %in% c("Baltimore, MD")) %>% 
    glm(resolved ~ victim_age + victim_sex +victim_race, data = ., family = binomial()) 

## Table below summaries the coefficients from the model fit
Baltimoe_fit_logistic%>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_Lower = exp(conf.low),
         CI_Upper = exp(conf.high)) %>%
  select(term, log_OR = estimate, OR, CI_Lower, CI_Upper, p.value) %>% 
  knitr::kable(align = "lrr",
               col.names = c("Term","Estimate","OR", "95% CI Lower", "95% CI Upper", "P-value"),
               digits = 3)
```
* In Baltimore, MD. Homicides in which the victim is Black are substantially less likely to be resolved that those in which the victim is white.
* In Baltimore, MD. Homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. 
* The effect of age is statistically significant, but careful data inspections should be conducted before interpreting too deeply.

```{r warning=FALSE}
# glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides 
allcities_logistic = homicides %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  mutate(term = fct_inorder(term),
         OR = exp(estimate),
         CI_Lower = exp(conf.low),
         CI_Upper = exp(conf.high)) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state,term, log_OR = estimate, OR, CI_Lower, CI_Upper, p.value)

allcities_logistic %>% 
  knitr::kable(align = "llccc",
               col.names = c("City_State","Term","Estimate","OR", "95% CI Lower", "95% CI Upper", "P-value"),
               digits = 3)
```

```{r}
## Create a plot that shows the estimated ORs and CIs for each city
allcities_logistic%>% 
   mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_Lower, ymax = CI_Upper)) +
  theme(axis.text.x = element_text(angle = 70, hjust = 1)) +
   labs(
    title = "Estimated ORs and CIs for Each City ",
    x = "City, State",
    y = "Estimated ORs"
  )

```

* Holding all other variables constant. For most cities, the OR is less than 1 indicates homicides in which the victim is male are significantly less like to be resolved than those in which the victim is female. However, there are some cities' CI interval includes 1(null value), meaning there is no significant difference between homicide cases is male to be resolved compare to female at the level of significance level of 0.05. 


## Problem 3

## clean the data 
```{r}
birthweight_raw = read_csv("./data/birthweight.csv")

birthweight =   birthweight_raw %>% 
  janitor::clean_names() %>%
  mutate(across(.cols = c( frace, malform, mrace, babysex,), as.factor)) %>%
  mutate(frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", 
                        "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
         mrace = recode(mrace, "1" = "White", "2" = "Black", 
                        "3" = "Asian", "4" = "Puerto Rican", "8" = "Other"),
         malform = ifelse(malform == "0", "absent","present"),
         babysex = ifelse(babysex == "1", "male","female"))
```

* **babysex, frace, malform, and mrace** are categorical variables, and I turned them into factors and also signed the information accroding to the assignment. 


## missing value check and summary dataset 

```{r}
skimr::skim(birthweight)
```

* No missing value in the data 

## propose a regression model for birthweight 

```{r,warning = FALSE, message = FALSE}
# stepwise regression 
mult.fit = lm(bwt ~ ., data = birthweight)
step(mult.fit, direction = "both", k = 2)
```

* Using the stepwise regression with  AIC, it determines the best model for birthweight is the one that minimizes the considered information criterion. The resulting model has 11 variables, which are `babysex`, `bhead`, `blength`, `delwt`, `fincome`, `gaweeks`, `mheight`, `mrace`, `parity`, `ppwt` and `smoken`.

```{r, warning=FALSE, message=FALSE}
model_fit1 = lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)
summary(model_fit1)
```
* From the table, I could tell the `fincome` has p value >0.05, so it's not significant at level of 0.05 and I will drop this variable. 

```{r, warning=FALSE, message=FALSE}
# explore the collinearity among selected continous variables. 
birthweight %>%
  select(bhead, blength, delwt, fincome, gaweeks, mheight, parity, ppwt, smoken) %>% 
  PerformanceAnalytics::chart.Correlation(method = "pearson")
```

* From the correlation scatter polt, we could conclude there is a potential collinearity between `delwt` and `ppwt` and also between `bhead` and `blength`. 
* Then, I will remove `ppwt` and `blength` from my model, whose p-values are larger in my model.

```{r, warning=FALSE, message=FALSE}
mult_fit_2 = lm(bwt ~  babysex + bhead + delwt  + gaweeks + mheight + mrace + parity + smoken,
              data=birthweight)
summary(mult_fit_2)
```

* In this new model, the `parity` has p value = 0.0803 which is higher than 0.05, meanding there is no association between parity and baby birthweight on the level of significance 0.05. So, i will drop `parity` as well. 
* My final model would be `bwt =  babysex + bhead + delwt  + gaweeks + mheight + mrace + smoken`.

## Model Comparision

```{r}
multi_fit_3 = lm(bwt ~ babysex + bhead + delwt + gaweeks + mheight + mrace + smoken, data = birthweight)
summary(multi_fit_3)
```
```{r, warning=FALSE, message=FALSE}
# Model using length at birth and gestational age as predictors (main effects only)
comp_fit_1 = lm(bwt ~ blength + gaweeks, data = birthweight)
# Model using head circumference, length, sex, and all interactions (including the three-way interaction) between these
comp_fit_2 = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = birthweight)

# cross-validation 
cv_df = crossv_mc(birthweight, 100) %>% 
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>% 
 mutate(
        model_1 = map(train, ~lm(bwt ~ babysex + bhead + delwt + gaweeks + mheight + mrace + smoken, data = as_tibble(.x))),
        model_2 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
        model_3 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(model_3, test, ~rmse(model = .x, data = .y)))
# root mean square error in 3 models 
cv_df %>% 
    select(starts_with("rmse")) %>% 
    pivot_longer(
      everything(),
      names_to = "model", 
      values_to = "rmse",
      names_prefix = "rmse_") %>% 
    mutate(model = fct_inorder(model)) %>% 
    ggplot(aes(x = model, y = rmse)) + 
    geom_boxplot() +
    labs(
         title = "Prediction Error Distributions across Models",
         x = "Models", 
         y = "Root Mean Square Error")+
    scale_x_discrete(
        labels = c("My Model", "Test Model 1", "Test Model 2")) +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"))
```

* From this boxplot, the test model 2 has the lowest RMSE, followed by my model and test model 1 has the highest RMSE. Therefore, test model 1 is potentially the best model among these three models, which we could do some futher investgation.
